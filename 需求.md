
# 开发一个web端应用，来为当前trae agent提供一个可视化的界面

# 页面形态
top区域：用户选择workspace，即选定一个本地目录，作为web应用的工作目录。trae agent将获取该工作空间的文件操作权限，并且后续的任务在该空间下直接操作。
主视觉区：
- 左侧：工作空间文件列表，不断读取用户本地目录的.md格式文件，并以列表的方式呈现，用户选择可以切换，在主视觉去显示这个文件的内容
- Markdown工作区域，显示文件内容，用户也可以自行编辑，并提供保存按钮。分为编辑区（用户进行编辑，markdonw未渲染样式）和预览区（显示渲染后的样式）
- 右侧：trae agent操作页面，包括输出trae agent运行过程中的输出、工具调用等信息，进行流式打印。用户需求输入对话框，用户可以在这里输入自然语言描述的需求，然后点击提交。请求提交至trae agent后台，agent开始工作。

# 机制：
trae agent工作，使用文件操作工具，将最终生成的结果写入文件，并实时预览到主视觉去的markdonw中。
trae agent工作过程中的推理信息、工具调用信息，lakeview信息，可以实时打印

# 请使用react框架，使用脚手架实现需求


# 前端改动需求：
用户自定义助手时，需要填入一下信息：
1、助手名称（模式名称）
2、助手prompt（模式prompt）
3、自定义指导
4、任务未完成消息
5、反思机制
并进行保存。后续选择该助手时，Agent代码执行时将在对应位置传入用户的自定义内容。
# 后台存储
以上数据使用Postgresql数据库存储。数据库固定端口，使用docker部署。
# Agent改动需求

当传入不同的prompt参数时，将同步影响以下相关信息，从而个性化trae agent运行机制。用户可以在前端页面自定义配置模式，模式包含以下几个方面：
1、模式名称 string 用户自定义模式名称，用于区分不同的运行模式。现在已经定义好了两个模式：文档助手、代码助手
2、模式prompt string
    代码位置：trae_agent/prompt/agent_prompt.py 
    用户在前端自定义模式prompt，代码中需要定一个可以灵活传入的 PROMPT={用户自定义的prompt}，在后续代码中被引用。现在已经定义好了两个prompt分别对应文档助手和代码助手。
3、自定义指导 string
    用户可以自定义一段指导语句，来指导LLM工作，这段指导语将会构建在usermessage中。
    
    代码位置：trae_agent/agent/trae_agent.py
    user_message = f"""
    [Project root path]:{self.project_path}
 
    [Problem statement]: We're currently solving the following issue within our repository. Here's the issue text:
    {extra_args['issue']}
 
    # 添加你的自定义指导
    [Special instructions]: 你的特殊指导语
    """
 
4、任务未完成消息
    代码位置：trae_agent/agent/trae_agent.py 的 task_incomplete_message 方法
    自定义指导语
5、反思机制
    代码位置：trae_agent/agent/trae_agent.py 的 reflect_on_result 方法 reflections.extend 下可以灵活传入用户自定义的反思语句




1、前端去掉模型配置选择文件上传功能以及url返回功能
2、将config yaml的模型信息去掉，包括 base url、model name、model uid、api key、provider等
3、config yaml 保留其他基础设置，lakeview设置为true，解决true导致的其他问题：
      enable_lakeview: true
      model: default_model
      max_steps: 50
      tools:
      - bash
      - str_replace_based_edit_tool
      - sequentialthinking
      - task_done
4、前端增加设置模型以及模型连通性测试的功能，需要选择模型客户端，填入base url、model name、model uid、api key 。并进行测试，通过后可保存，并在外层选中模型，后续该会话提交的请求将使用这个模型配置。并按照以下配置默认初始化一个可选模型：
model_providers:
  openrouter:
    api_key: "sk-xinference"
    provider: openrouter
    base_url: http://10.0.2.22:9997/v1
models:
  qwen3:
    model: Qwen3-32B
    model_provider: openrouter
    temperature: 0.2
    top_p: 1
    top_k: 0
    parallel_tool_calls: true
    max_retries: 3
    客户端：Xinference

根据需求，以及现在项目现状（后端api、前端、docker部署环境、网络通信问题），首先梳理出清楚的需求与解决方案。如有问题，必须首先询问我的意见。最终进行实施